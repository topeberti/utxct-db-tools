{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58acffe0",
   "metadata": {},
   "source": [
    "# Experiment Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017e255",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee0b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from tabulate import tabulate\n",
    "#import a folder in the parent directory\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import dbtools.dbtools as qrs\n",
    "import dbtools.load as load\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72cfd18",
   "metadata": {},
   "source": [
    "## Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd40bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to the PostgreSQL database\n",
    "    conn = qrs.connect()\n",
    "    print(\"Connected to the database\")\n",
    "\n",
    "except (Exception, psycopg2.DatabaseError) as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e4932",
   "metadata": {},
   "source": [
    "# Loading experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a031a7",
   "metadata": {},
   "source": [
    "We want to load old experiments, so we will use the path to the folder to also extract all the models and load them into the database too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2967f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = Path(r'\\\\192.168.10.106\\imdea\\DataDriven_UT_AlbertoVicente\\05_Models\\Juan Ignacio\\2025\\CNN\\IQ\\10 mm\\top3_KFold7x7')\n",
    "\n",
    "# description = \"Top 3 configurations are trained using a KFold with K=5, each model is trained 5 times in each Fold. Then they are ranked based on the average of each metric. \" \\\n",
    "# \"Single signal inputed, signal type IQ, 10mm signal range\" \\\n",
    "# \"Signals are aligned arround the same point 50\" \\\n",
    "# \"Volfrac is divided in bins using np.histogram with auto in number of bins, then augmentation is applied to each bin until it reaches the same number of samples as the biggest bin.\" \\\n",
    "# \"Data is divided in train test and sample. Using 1 sample for test, another for validation and the rest for training.\" \\\n",
    "# \"Data is normalized based on training data, using minmax normalization for signals and volfrac.\" \\\n",
    "# \"When sampled from the datagenerator each signal is randomly rolled, that is how the augmentation is applied.\" \\\n",
    "# \"Each model is trained 5 times to get an average of the performance.\"\n",
    "\n",
    "# description = \"Top 10 bayesian found configurations are now trained 5 times each. Then they are ranked based on the average of each metric. \" \\\n",
    "# \"Single signal inputed, signal type IQ, 10mm signal range\" \\\n",
    "# \"Signals are aligned arround the same point 50\" \\\n",
    "# \"Volfrac is divided in bins using np.histogram with auto in number of bins, then augmentation is applied to each bin until it reaches the same number of samples as the biggest bin.\" \\\n",
    "# \"Data is divided in train test and sample. Using 1 sample for test, another for validation and the rest for training.\" \\\n",
    "# \"Data is normalized based on training data, using minmax normalization for signals and volfrac.\" \\\n",
    "# \"When sampled from the datagenerator each signal is randomly rolled, that is how the augmentation is applied.\" \\\n",
    "# \"Each model is trained 5 times to get an average of the performance.\"\n",
    "\n",
    "description = [\"Hyperband + Bayesian search. The top 10 configurations of the hyperband search are used to create the search space for Bayesian search, using the max an min of each as bounds.\" \\\n",
    "               \"Test and validation is created by separating two samples from the dataset, then mixing them and splitting 80/20\"]\n",
    "\n",
    "author = \"Alberto Vicente del Egido\"\n",
    "\n",
    "dataset_paths = [str(Path(path)) for path in [\n",
    "    r'\\\\192.168.10.106\\imdea\\DataDriven_UT_AlbertoVicente\\04_ML_data\\Juan Ignacio\\10mm range\\JI_4\\MonoElement\\patch_vs_volfrac_7.csv',\n",
    "    r'\\\\192.168.10.106\\imdea\\DataDriven_UT_AlbertoVicente\\04_ML_data\\Juan Ignacio\\10mm range\\JI_5\\MonoElement\\patch_vs_volfrac_7.csv',\n",
    "    r'\\\\192.168.10.106\\imdea\\DataDriven_UT_AlbertoVicente\\04_ML_data\\Juan Ignacio\\10mm range\\JI_7\\MonoElement\\patch_vs_volfrac_7.csv',\n",
    "    r'\\\\192.168.10.106\\imdea\\DataDriven_UT_AlbertoVicente\\04_ML_data\\Juan Ignacio\\10mm range\\JI_8\\MonoElement\\patch_vs_volfrac_7.csv',\n",
    "    r'\\\\192.168.10.106\\imdea\\DataDriven_UT_AlbertoVicente\\04_ML_data\\Juan Ignacio\\10mm range\\JI_10\\MonoElement\\patch_vs_volfrac_7.csv',\n",
    "    r'\\\\192.168.10.106\\imdea\\DataDriven_UT_AlbertoVicente\\04_ML_data\\Juan Ignacio\\10mm range\\JI_11\\MonoElement\\patch_vs_volfrac_7.csv',\n",
    "    r'\\\\192.168.10.106\\imdea\\DataDriven_UT_AlbertoVicente\\04_ML_data\\Juan Ignacio\\10mm range\\JI_12\\MonoElement\\patch_vs_volfrac_7.csv'\n",
    "]]\n",
    "\n",
    "aditional_metadata = {\n",
    "    \"hyperband_objective\": \"val_loss\",\n",
    "    \"hyperband_max_epochs\": 200,\n",
    "    \"hyperband_factor\":3,\n",
    "    \"hyperband_iterations\":5,\n",
    "    \"hyperband_executions_per_trial\":5,\n",
    "    \"bayesian_trials\": 100,\n",
    "    \"bayesian_warmup\":5,\n",
    "    \"bayesian_executions_per_trial\":5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load.load_experiment(conn,str(experiment_path),description,author,dataset_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf7c30",
   "metadata": {},
   "source": [
    "# Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaecb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the folders in the experiment path\n",
    "folders = [f for f in experiment_path.iterdir() if f.is_dir()]\n",
    "\n",
    "#list only the folders that contain the word 'model'\n",
    "folders = [f for f in folders if 'model' in f.name]\n",
    "\n",
    "#names of the folders are in this format: 'model_1', 'model_2', etc. Order them by the number\n",
    "folders.sort(key=lambda x: int(x.name.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = '1D CNN'\n",
    "description = '1D convolutional neural network, single signal input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folders:\n",
    "    #in the folder there is a file called 'model_info.txt'\n",
    "    model_info_path = folder / 'model_info.txt'\n",
    "    try:\n",
    "        #read the file\n",
    "        with open(model_info_path, 'r') as f:\n",
    "            model_info = f.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {model_info_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # the file is in this format: every line is key:value, but value may contain ':'\n",
    "    model_info_dict = {}\n",
    "    for line in model_info.split('\\n'):\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            model_info_dict[key.strip()] = value.strip()\n",
    "    \n",
    "    try:\n",
    "        parameters = int(model_info_dict.get('Model Parameters', ''))\n",
    "        trainable_parameters = int(model_info_dict.get('Trainable Parameters', ''))\n",
    "        metrics = {}\n",
    "        mse = float(model_info_dict.get('Mean Squared Error', '0'))\n",
    "        metrics['mse'] = mse\n",
    "        rmse = float(model_info_dict.get('Root Mean Squared Error', '0'))\n",
    "        metrics['rmse'] = rmse\n",
    "        mae = float(model_info_dict.get('Mean Absolute Error', '0'))\n",
    "        metrics['mae'] = mae\n",
    "        coverage = float(model_info_dict.get('Coverage', '0'))\n",
    "        metrics['coverage'] = coverage\n",
    "        bias = float(model_info_dict.get('Bias', '0'))\n",
    "        metrics['bias'] = bias\n",
    "        r2 = float(model_info_dict.get('R2', '0'))\n",
    "        metrics['r2'] = r2\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing model info for {model_info_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    load.load_model(conn,experiment_folder_path=str(experiment_path), model_folder_path=str(folder), architecture=architecture, description=description,parameters=parameters,trainable_parameters=trainable_parameters,computed_metrics=metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
